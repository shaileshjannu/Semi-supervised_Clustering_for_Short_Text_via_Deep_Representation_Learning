{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflowで\"Semi-supervised Clustering for Short Text via Deep Representation Learning\"の実装¶\n",
    "http://aclweb.org/anthology/K16-1004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import random as rd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tqdm import tqdm\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import MeCab\n",
    "import subprocess\n",
    "import itertools\n",
    "import string\n",
    "import sqlite3\n",
    "from keras.preprocessing import sequence\n",
    "rng = np.random.RandomState(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#ここで各ファイルのパスを設定します。\n",
    "#\n",
    "\n",
    "#学習に用いるデータ\n",
    "dbpath = \"../input/category_walkerplus.db\"\n",
    "\n",
    "#word2vecのモデル\n",
    "model_path=\"../input/word2vec.gensim.model\"\n",
    "\n",
    "#辞書\n",
    "dic_path=\"/usr/local/lib/mecab/dic/mecab-ipadic-neologd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(model_path)\n",
    "tagger = MeCab.Tagger(\"-Ochasen -d {0}\".format(dic_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cur(sourcedbname):\n",
    "    con = sqlite3.connect(sourcedbname)\n",
    "    cur = con.cursor()   \n",
    "    sql = \"select * from events\"\n",
    "    cur.execute(sql)\n",
    "    return cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cur=cur(dbpath)\n",
    "\n",
    "labels=[]\n",
    "texts=[]\n",
    "for row in Cur:\n",
    "    labels.append(row[0].replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")[1].replace(\" \",\"\"))  #Big Category\n",
    "    texts.append(row[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分散表現の獲得\n",
    "Data：入力の文章を分かち書きしw2vで埋め込み後padding shape=(データ数,maxlen,埋め込み次元)<br/>\n",
    "Labels:正解ラベル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分かち書き"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _tokenize(text):\n",
    "    sentence = []\n",
    "    node = tagger.parse(text)\n",
    "    #print node\n",
    "    node = node.split(\"\\n\")\n",
    "    for i in range(len(node)):\n",
    "        feature = node[i].split(\"\\t\")\n",
    "        if feature[0] == \"EOS\":\n",
    "            break\n",
    "        hinshi = feature[3].split(\"-\")[0]\n",
    "        if \"名詞\" in hinshi:\n",
    "            #sentence.append(feature[2].decode('utf-8'))\n",
    "            sentence.append(feature[2])\n",
    "        elif \"形容詞\" in hinshi:\n",
    "            #sentence.append(feature[2].decode('utf-8'))\n",
    "            sentence.append(feature[2])\n",
    "        elif \"動詞\" in hinshi:\n",
    "            #sentence.append(feature[2].decode('utf-8'))\n",
    "            sentence.append(feature[2])\n",
    "        elif \"形容動詞\" in hinshi:\n",
    "            #sentence.append(feature[2].decode('utf-8'))\n",
    "            sentence.append(feature[2])\n",
    "        elif \"連体詞\" in hinshi:\n",
    "            #sentence.append(feature[2].decode('utf-8'))\n",
    "            sentence.append(feature[2])           \n",
    "        elif \"助詞\" in hinshi:\n",
    "            #sentence.append(feature[2].decode('utf-8'))\n",
    "            sentence.append(feature[2])\n",
    "            \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分散表現の獲得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getVector(text):\n",
    "    texts = _tokenize(text)\n",
    "    v = []\n",
    "    for t in texts:\n",
    "        if t in model.wv:\n",
    "            if v == []:\n",
    "                v = model.wv[t]\n",
    "            else:\n",
    "                v = np.vstack((v,model.wv[t]))\n",
    "    if v != []:\n",
    "        return v\n",
    "    else:\n",
    "        return np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomoki/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/ipykernel/__main__.py:6: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "/Users/tomoki/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/ipykernel/__main__.py:10: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "Data=np.array([getVector(text) for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Data = sequence.pad_sequences(Data, padding=\"post\", truncating=\"post\",dtype=\"float32\")  #padding  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label2Label={name:i for i,name in enumerate(np.unique(labels))}  #label(名前)->Label(数字)\n",
    "Labels=np.array([label2Label[label] for label in labels])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.教師データの選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(Data,Labels,training_percent=0.9, supervise_percent=0.1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Data：入力の文章を分かち書きしw2vで埋め込み後padding shape=(データ数,maxlen,埋め込み次元)\n",
    "        Labels:正解ラベル\n",
    "        training_percent:trainingデータに使用する割合\n",
    "        supervise_percent: supervise(教師データ)として一部与える割合(training_dataに対して)\n",
    "    Returns:\n",
    "        train_X:Dataの学習用\n",
    "        train_y:Labelsの学習用\n",
    "        test_X:Dataのテスト用\n",
    "        test_y:Labelsのテスト用\n",
    "        supervised:学習用データにおいて一部与える教師データのindex\n",
    "    \"\"\"\n",
    "    #trainとtestでclassが均等になるようにsplit\n",
    "    cluster_num=np.unique(Labels)\n",
    "    train_index = []\n",
    "    \n",
    "    for i in range(len(cluster_num)):\n",
    "        num = Labels[Labels==i].shape[0]\n",
    "        k = int(num*training_percent)\n",
    "        train_index.extend(rd.sample(list(np.where(Labels==i)[0]),k))\n",
    "    \n",
    "    #text classification for cnn のためにあらかじめlayerを追加\n",
    "    train_X = Data[train_index][:,:,:,np.newaxis]\n",
    "    train_y = Labels[train_index]\n",
    "    test_X = np.delete(Data,train_index,0)[:,:,:,np.newaxis]\n",
    "    test_y = np.delete(Labels,train_index,0)\n",
    "    \n",
    "    supervised = []\n",
    "\n",
    "    for i in range(len(cluster_num)):\n",
    "        num = train_y[train_y==i].shape[0]\n",
    "        k = int(num*supervise_percent)\n",
    "       \n",
    "        supervised.extend(rd.sample(list(np.where(train_y==i)[0]),k))\n",
    "\n",
    "    \n",
    "    return train_X,train_y,test_X,test_y,supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X,train_y,test_X,test_y, supervised = load_data(Data,Labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.CNN for Text Classificationの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#窓の幅\n",
    "filter_sizes = [3,5,7]\n",
    "#分散表現の次元\n",
    "vector_length = train_X.shape[2]\n",
    "#最大系列長\n",
    "sequence_length = train_X.shape[1]\n",
    "#フィルターの枚数\n",
    "num_filters = 16\n",
    "#隠れ層\n",
    "hid_dim=100\n",
    "#出力次元数\n",
    "output_dim=2\n",
    "#クラスタリングするクラス多数\n",
    "n_cluster=len(np.unique(train_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Conv:\n",
    "    def __init__(self, sequence_length,embedding_size,filter_sizes, num_filters):\n",
    "        self.sequence_length=sequence_length\n",
    "        self.embedding_size=embedding_size\n",
    "        self.filter_sizes=filter_sizes\n",
    "        self.num_filters=num_filters\n",
    "    def f_prop(self,x):\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(self.filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, self.embedding_size, 1, self.num_filters]\n",
    "                \n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    x,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                \n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, self.sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(self.filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        return self.h_pool_flat\n",
    "\n",
    "class Dense:\n",
    "    def __init__(self, in_dim, out_dim, function=lambda x: x):\n",
    "        # Xavier initializer\n",
    "        self.W = tf.Variable(rng.uniform(\n",
    "                        low=-np.sqrt(6/(in_dim + out_dim)),\n",
    "                        high=np.sqrt(6/(in_dim + out_dim)),\n",
    "                        size=(in_dim, out_dim)\n",
    "                    ).astype('float32'), name='W')\n",
    "        self.b = tf.Variable(np.zeros([out_dim]).astype('float32'))\n",
    "        self.function = function\n",
    "\n",
    "    def f_prop(self, x):\n",
    "        return self.function(tf.matmul(x, self.W) + self.b)\n",
    "        \n",
    "class LinearDense:\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        # Xavier initializer\n",
    "        self.W = tf.Variable(rng.uniform(\n",
    "                        low=-np.sqrt(6/(in_dim + out_dim)),\n",
    "                        high=np.sqrt(6/(in_dim + out_dim)),\n",
    "                        size=(in_dim, out_dim)\n",
    "                    ).astype('float32'), name='W')\n",
    "        self.b = tf.Variable(np.zeros([out_dim]).astype('float32'))\n",
    "\n",
    "    def f_prop(self, x):\n",
    "        return tf.matmul(x, self.W) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### グラフの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_x = tf.placeholder(tf.float32, [None, sequence_length,vector_length,1], name=\"input_x\")\n",
    "input_t = tf.placeholder(tf.float32, [None, output_dim], name=\"input_y\")\n",
    "\n",
    "layers = [\n",
    "    Conv(sequence_length=sequence_length,\n",
    "         embedding_size=vector_length,\n",
    "         filter_sizes=filter_sizes,\n",
    "         num_filters=num_filters),\n",
    "    Dense(num_filters * len(filter_sizes),hid_dim , tf.nn.tanh),\n",
    "    LinearDense(hid_dim, output_dim)\n",
    "]\n",
    "\n",
    "def f_props(layers, x):\n",
    "    for i, layer in enumerate(layers):\n",
    "        x = layer.f_prop(x)\n",
    "    return x\n",
    "\n",
    "pred_y = f_props(layers, input_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 誤差関数の設計\n",
    "<img src='../img/jsemi.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#目的関数の定義\n",
    "def _cost(pred_y, centers, neighbor_index, sup_index, mask ,alpha=0.01,l=0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pred_y: text-cnn の出力\n",
    "        centers:各ラベルごとの重心 shape=(cluster_num, output_dim)\n",
    "        neighbor_index:train_Xがどの重心に最も近いか shape=(data_num,)\n",
    "        sup_index:教師データとして用いるtrain_Xのindex shape=(sup_num,)\n",
    "        mask:教師データとして用いるindexに1、それ以外に0が入ったmask\n",
    "    \"\"\"\n",
    "    \n",
    "    term1= tf.reduce_sum(tf.square(pred_y - tf.gather(centers, neighbor_index)))    \n",
    "    term1_1 = alpha*tf.cast(term1, tf.float32)\n",
    "    \n",
    "    \n",
    "    term2 = tf.reduce_sum(mask * tf.square(pred_y - tf.gather(centers, sup_index) ))\n",
    "    term2_1 = (1-alpha)*tf.cast(term2, tf.float32)\n",
    "\n",
    "    cost = tf.add(term1_1, term2_1)\n",
    "    \n",
    "    for i in range(n_cluster):\n",
    "        i_index = i * tf.ones_like(sup_index, dtype='int32')\n",
    "        \n",
    "        x1 = tf.reduce_sum(tf.square(pred_y - tf.gather(centers, sup_index)),1)#正解の重心との距離\n",
    "        x2 = tf.reduce_sum(tf.square(pred_y - tf.gather(centers, i_index)),1)     #i番目の重心との距離\n",
    "                \n",
    "        term2_2 = l+x1-x2\n",
    "        \n",
    "        condition = tf.greater(term2_2, 0)\n",
    "        term2_3 = tf.reduce_sum(mask * tf.where(condition, term2_2, tf.zeros_like(term2_2)))\n",
    "        term2_3 = tf.cast(term2_3, tf.float32)\n",
    "\n",
    "        cost=tf.add(cost, (1-alpha)*term2_3)\n",
    "        \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  centers:各ラベルごとの重心 shape=(cluster_num, output_dim) \n",
    "centers= tf.placeholder(tf.float32, [None, output_dim], name=\"centers\")\n",
    "\n",
    "#  neighbor_index:train_Xがどの重心に最も近いか shape=(data_num,)\n",
    "neighbor_index= tf.placeholder(tf.int32, [None], name=\"neighbor_index\")\n",
    "\n",
    "#  sup_index:教師データとして用いるtrain_Xのindex shape=(sup_num,)\n",
    "sup_index=tf.placeholder(tf.int32, [None], name=\"supervised_index\")\n",
    "\n",
    "#  mask:教師データとして用いるindexに1、それ以外に0が入ったmask\n",
    "mask=tf.placeholder(tf.float32, [None,1], name=\"mask\")\n",
    "\n",
    "#自作の誤差関数\n",
    "cost = _cost(pred_y, centers, neighbor_index, sup_index, mask)\n",
    "\n",
    "# 最小化にはAdamを用いる\n",
    "train = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Iteration\n",
    "<img src='../img/iter.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def assign_to_nearest(samples, centroids):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        samples:text-cnn後の出力\n",
    "        centroids:クラスターそれぞれの重心 shape=(cluster_num,output_dim)\n",
    "    Returns:\n",
    "        nearest: 入力データと最も近いクラスターID shape=(data_num, )\n",
    "    \"\"\"\n",
    "    #1-1.KNearest Neighborで一番近いクラスタと紐付け\n",
    "    neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "    neigh.fit(centroids, np.arange(len(centroids)))\n",
    "    nearest = neigh.predict(samples)\n",
    "    \n",
    "    #1-2.ハンガリアンアルゴリズムでラベル付きデータと重心を紐付ける\n",
    "    \n",
    "    sup_labels =train_y[supervised]\n",
    "    hglabel = np.unique(sup_labels)\n",
    "    \n",
    "    hgx=[]  #教師データのラベルごとに平均した点（重心）を求める\n",
    "    for i in hglabel:\n",
    "        ind=np.where(sup_labels==i)[0]\n",
    "        hgx.append(np.mean(samples[supervised][ind], axis=0))\n",
    "    hgx = np.array(hgx)\n",
    "\n",
    "    #教師ラベルごとの重心と現在の重心との距離行列\n",
    "    DistanceMatrix = np.linalg.norm(hgx[:,np.newaxis,:]-centroids[np.newaxis,:,:],axis=2)  \n",
    "    \n",
    "    # ハンガリアンアルゴリズムで合計が一番小さくなるように紐づける \n",
    "    from scipy.optimize import linear_sum_assignment\n",
    "    row_ind, col_ind = linear_sum_assignment(DistanceMatrix)\n",
    "    \n",
    "    #ラベルとclusterIDを紐づける\n",
    "    label2id={hglabel[i]:col for i,col in enumerate(col_ind)}\n",
    "    \n",
    "    \n",
    "    return nearest ,label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def estimate_centroids(samples, nearest_indices, sup_cent,centroids,label2id, use_supervised=True):\n",
    "    \"\"\"\n",
    "    重心を再推定する\n",
    "    重心の再推定式は簡略化してラベル付きと無しの加重平均\n",
    "    Args:\n",
    "        samples:text-cnn後の出力\n",
    "        nearest: 入力データと最も近いクラスターID shape=(data_num, )\n",
    "    Returns:\n",
    "        centroids:クラスターそれぞれの重心 shape=(cluster_num,output_dim)\n",
    "    \"\"\"\n",
    "    sup_pred=samples[supervised]\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        sum1 = np.sum(alpha*len(np.where(nearest_indices==i)[0]))\n",
    "        sum2 = np.sum(alpha*samples[nearest_indices==i], axis=0)\n",
    "    \n",
    "        if use_supervised:\n",
    "            newce=np.array([label2id[t] for t in train_y[supervised]])\n",
    "            np.sum(samples[newce], axis=0)\n",
    "            sum33 = np.sum((1-alpha)*len(np.where(newce==kk)[0]))\n",
    "            sum44 = np.sum((1-alpha)*sup_pred[newce==kk], axis=0)\n",
    "            \n",
    "            centroids[kk] = (sum2+sum44)/(sum1+sum33)\n",
    "            \n",
    "        else:\n",
    "            centroids[kk] = (sum2)/(sum1)\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    #3.NNのパラメータ更新\n",
    "    sup_cent = []\n",
    "    Data.ix[supervised,\"ID\"].apply(lambda x:sup_cent.append(list(centroids[label2id[x]])))\n",
    "    \n",
    "    return np.array(centroids) , np.array(sup_cent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
